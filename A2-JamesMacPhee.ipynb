{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James MacPhee\n",
      "B00768516\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freq</th>\n",
       "      <th>PMI</th>\n",
       "      <th>t-test</th>\n",
       "      <th>Chi-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((Subject, NNP), (Re, NNP))</td>\n",
       "      <td>((Evelyn, NNP), (Conlon, NNP))</td>\n",
       "      <td>((Subject, NNP), (Re, NNP))</td>\n",
       "      <td>((ALink, NNP), (KSAND, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((Organization, NNP), (University, NNP))</td>\n",
       "      <td>((Duck, NNP), (Pond, NNP))</td>\n",
       "      <td>((Organization, NNP), (University, NNP))</td>\n",
       "      <td>((Carnegie, NNP), (Mellon, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((Lines, NNP), (NNTPPostingHost, NNP))</td>\n",
       "      <td>((decaycbnewsjcbattcom, NN), (deankaflowitz, NN))</td>\n",
       "      <td>((Lines, NNP), (NNTPPostingHost, NNP))</td>\n",
       "      <td>((Cookamunga, NNP), (Tourist, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((Distribution, NNP), (world, NN))</td>\n",
       "      <td>((ancient, NN), (Mayans, NNPS))</td>\n",
       "      <td>((Distribution, NNP), (world, NN))</td>\n",
       "      <td>((Evelyn, NNP), (Conlon, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((Lines, NNP), (Distribution, NNP))</td>\n",
       "      <td>((Notre, NNP), (Dame, NNP))</td>\n",
       "      <td>((Lines, NNP), (Distribution, NNP))</td>\n",
       "      <td>((Notre, NNP), (Dame, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>((world, NN), (NNTPPostingHost, NNP))</td>\n",
       "      <td>((Eau, NNP), (Claire, NNP))</td>\n",
       "      <td>((world, NN), (NNTPPostingHost, NNP))</td>\n",
       "      <td>((OriginalSender, NNP), (isuVACATIONVENARICSCM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>((Henry, NNP), (Spencer, NNP))</td>\n",
       "      <td>((Tape, NNP), (Cites, NNP))</td>\n",
       "      <td>((Henry, NNP), (Spencer, NNP))</td>\n",
       "      <td>((fait, NN), (comme, NN))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>((Computer, NNP), (Science, NNP))</td>\n",
       "      <td>((Frequently, NNP), (Asked, NNP))</td>\n",
       "      <td>((Computer, NNP), (Science, NNP))</td>\n",
       "      <td>((Eau, NNP), (Claire, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>((TIN, NNP), (version, NN))</td>\n",
       "      <td>((Southwestern, NNP), (Louisiana, NNP))</td>\n",
       "      <td>((TIN, NNP), (version, NN))</td>\n",
       "      <td>((Duck, NNP), (Pond, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>((version, NN), (PL, NNP))</td>\n",
       "      <td>((fait, NN), (comme, NN))</td>\n",
       "      <td>((version, NN), (PL, NNP))</td>\n",
       "      <td>((Mantis, NNP), (Consultants, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>((XNewsreader, NNP), (TIN, NNP))</td>\n",
       "      <td>((sank, JJ), (Manhattan, NNP))</td>\n",
       "      <td>((XNewsreader, NNP), (TIN, NNP))</td>\n",
       "      <td>((Los, NNP), (Angeles, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>((Jon, NNP), (Livesey, NNP))</td>\n",
       "      <td>((GENERAL, NNP), (UNIFIED, NNP))</td>\n",
       "      <td>((Jon, NNP), (Livesey, NNP))</td>\n",
       "      <td>((VAXVMS, NNP), (VNEWS, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>((Does, NNP), (anyone, NN))</td>\n",
       "      <td>((Steinn, NNP), (Sigurdsson, NNP))</td>\n",
       "      <td>((Does, NNP), (anyone, NN))</td>\n",
       "      <td>((sank, JJ), (Manhattan, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>((Apr, NNP), (GMT, NNP))</td>\n",
       "      <td>((Beam, NNP), (Jockey, NNP))</td>\n",
       "      <td>((Apr, NNP), (GMT, NNP))</td>\n",
       "      <td>((ISLAMIC, NNP), (LAW, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>((Kent, NNP), (Sandvik, NNP))</td>\n",
       "      <td>((icsucieduincominggeodegif, NN), (icsucieduin...</td>\n",
       "      <td>((Kent, NNP), (Sandvik, NNP))</td>\n",
       "      <td>((Steinn, NNP), (Sigurdsson, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>((State, NNP), (University, NNP))</td>\n",
       "      <td>((bzawutarlgutaedu, NN), (stephen, NN))</td>\n",
       "      <td>((State, NNP), (University, NNP))</td>\n",
       "      <td>((Beam, NNP), (Jockey, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>((PL, NNP), (Lines, NNP))</td>\n",
       "      <td>((Asked, NNP), (Questions, NNP))</td>\n",
       "      <td>((PL, NNP), (Lines, NNP))</td>\n",
       "      <td>((icsucieduincominggeodegif, NN), (icsucieduin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>((Frank, NNP), (ODwyer, NNP))</td>\n",
       "      <td>((MY, NNP), (REPLY, NNP))</td>\n",
       "      <td>((Frank, NNP), (ODwyer, NNP))</td>\n",
       "      <td>((GENERAL, NNP), (UNIFIED, NNP))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>((Toronto, NNP), (Zoology, NNP))</td>\n",
       "      <td>((Dryden, NNP), (FredMcCalldsegticom, NNP))</td>\n",
       "      <td>((Toronto, NNP), (Zoology, NNP))</td>\n",
       "      <td>((decaycbnewsjcbattcom, NN), (deankaflowitz, NN))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>((University, NNP), (Lines, NNP))</td>\n",
       "      <td>((ISLAMIC, NNP), (LAW, NNP))</td>\n",
       "      <td>((Jesus, NNP), (Christ, NNP))</td>\n",
       "      <td>((Tourist, NNP), (Bureau, NNP))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Freq  \\\n",
       "0                ((Subject, NNP), (Re, NNP))   \n",
       "1   ((Organization, NNP), (University, NNP))   \n",
       "2     ((Lines, NNP), (NNTPPostingHost, NNP))   \n",
       "3         ((Distribution, NNP), (world, NN))   \n",
       "4        ((Lines, NNP), (Distribution, NNP))   \n",
       "5      ((world, NN), (NNTPPostingHost, NNP))   \n",
       "6             ((Henry, NNP), (Spencer, NNP))   \n",
       "7          ((Computer, NNP), (Science, NNP))   \n",
       "8                ((TIN, NNP), (version, NN))   \n",
       "9                 ((version, NN), (PL, NNP))   \n",
       "10          ((XNewsreader, NNP), (TIN, NNP))   \n",
       "11              ((Jon, NNP), (Livesey, NNP))   \n",
       "12               ((Does, NNP), (anyone, NN))   \n",
       "13                  ((Apr, NNP), (GMT, NNP))   \n",
       "14             ((Kent, NNP), (Sandvik, NNP))   \n",
       "15         ((State, NNP), (University, NNP))   \n",
       "16                 ((PL, NNP), (Lines, NNP))   \n",
       "17             ((Frank, NNP), (ODwyer, NNP))   \n",
       "18          ((Toronto, NNP), (Zoology, NNP))   \n",
       "19         ((University, NNP), (Lines, NNP))   \n",
       "\n",
       "                                                  PMI  \\\n",
       "0                      ((Evelyn, NNP), (Conlon, NNP))   \n",
       "1                          ((Duck, NNP), (Pond, NNP))   \n",
       "2   ((decaycbnewsjcbattcom, NN), (deankaflowitz, NN))   \n",
       "3                     ((ancient, NN), (Mayans, NNPS))   \n",
       "4                         ((Notre, NNP), (Dame, NNP))   \n",
       "5                         ((Eau, NNP), (Claire, NNP))   \n",
       "6                         ((Tape, NNP), (Cites, NNP))   \n",
       "7                   ((Frequently, NNP), (Asked, NNP))   \n",
       "8             ((Southwestern, NNP), (Louisiana, NNP))   \n",
       "9                           ((fait, NN), (comme, NN))   \n",
       "10                     ((sank, JJ), (Manhattan, NNP))   \n",
       "11                   ((GENERAL, NNP), (UNIFIED, NNP))   \n",
       "12                 ((Steinn, NNP), (Sigurdsson, NNP))   \n",
       "13                       ((Beam, NNP), (Jockey, NNP))   \n",
       "14  ((icsucieduincominggeodegif, NN), (icsucieduin...   \n",
       "15            ((bzawutarlgutaedu, NN), (stephen, NN))   \n",
       "16                   ((Asked, NNP), (Questions, NNP))   \n",
       "17                          ((MY, NNP), (REPLY, NNP))   \n",
       "18        ((Dryden, NNP), (FredMcCalldsegticom, NNP))   \n",
       "19                       ((ISLAMIC, NNP), (LAW, NNP))   \n",
       "\n",
       "                                      t-test  \\\n",
       "0                ((Subject, NNP), (Re, NNP))   \n",
       "1   ((Organization, NNP), (University, NNP))   \n",
       "2     ((Lines, NNP), (NNTPPostingHost, NNP))   \n",
       "3         ((Distribution, NNP), (world, NN))   \n",
       "4        ((Lines, NNP), (Distribution, NNP))   \n",
       "5      ((world, NN), (NNTPPostingHost, NNP))   \n",
       "6             ((Henry, NNP), (Spencer, NNP))   \n",
       "7          ((Computer, NNP), (Science, NNP))   \n",
       "8                ((TIN, NNP), (version, NN))   \n",
       "9                 ((version, NN), (PL, NNP))   \n",
       "10          ((XNewsreader, NNP), (TIN, NNP))   \n",
       "11              ((Jon, NNP), (Livesey, NNP))   \n",
       "12               ((Does, NNP), (anyone, NN))   \n",
       "13                  ((Apr, NNP), (GMT, NNP))   \n",
       "14             ((Kent, NNP), (Sandvik, NNP))   \n",
       "15         ((State, NNP), (University, NNP))   \n",
       "16                 ((PL, NNP), (Lines, NNP))   \n",
       "17             ((Frank, NNP), (ODwyer, NNP))   \n",
       "18          ((Toronto, NNP), (Zoology, NNP))   \n",
       "19             ((Jesus, NNP), (Christ, NNP))   \n",
       "\n",
       "                                          Chi-squared  \n",
       "0                        ((ALink, NNP), (KSAND, NNP))  \n",
       "1                    ((Carnegie, NNP), (Mellon, NNP))  \n",
       "2                 ((Cookamunga, NNP), (Tourist, NNP))  \n",
       "3                      ((Evelyn, NNP), (Conlon, NNP))  \n",
       "4                         ((Notre, NNP), (Dame, NNP))  \n",
       "5   ((OriginalSender, NNP), (isuVACATIONVENARICSCM...  \n",
       "6                           ((fait, NN), (comme, NN))  \n",
       "7                         ((Eau, NNP), (Claire, NNP))  \n",
       "8                          ((Duck, NNP), (Pond, NNP))  \n",
       "9                 ((Mantis, NNP), (Consultants, NNP))  \n",
       "10                       ((Los, NNP), (Angeles, NNP))  \n",
       "11                      ((VAXVMS, NNP), (VNEWS, NNP))  \n",
       "12                     ((sank, JJ), (Manhattan, NNP))  \n",
       "13                       ((ISLAMIC, NNP), (LAW, NNP))  \n",
       "14                 ((Steinn, NNP), (Sigurdsson, NNP))  \n",
       "15                       ((Beam, NNP), (Jockey, NNP))  \n",
       "16  ((icsucieduincominggeodegif, NN), (icsucieduin...  \n",
       "17                   ((GENERAL, NNP), (UNIFIED, NNP))  \n",
       "18  ((decaycbnewsjcbattcom, NN), (deankaflowitz, NN))  \n",
       "19                    ((Tourist, NNP), (Bureau, NNP))  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"James MacPhee\")\n",
    "print(\"B00768516\")\n",
    "\n",
    "#Q1\n",
    "#(a)\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_data = fetch_20newsgroups(subset='all', shuffle=True, categories=['alt.atheism','talk.religion.misc','comp.graphics','sci.space'])\n",
    "data = pd.Series(twenty_data.data).astype(str)\n",
    "\n",
    "def _removeNonAscii(s): \n",
    "    return \"\".join(i for i in s if (ord(i)<123 and ord(i)>96) or (ord(i)<91 and ord(i)>64) or (ord(i)==32) or (ord(i)==10))\n",
    "clean_data = data.map(lambda x: _removeNonAscii(x))\n",
    "\n",
    "STOPWORDS_DICT = {lang: set(nltk.corpus.stopwords.words(lang)) for lang in nltk.corpus.stopwords.fileids()}\n",
    "\n",
    "def get_language(text):\n",
    "    words = set(nltk.wordpunct_tokenize(text.lower()))\n",
    "    lang = max(((lang, len(words & stopwords)) for lang, stopwords in STOPWORDS_DICT.items()), key = lambda x: x[1])[0]\n",
    "    if lang == 'english':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "english_data=clean_data[clean_data.apply(get_language)]\n",
    "english_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "token_data = []\n",
    "for i in range(len(english_data)):\n",
    "    if english_data[i]:\n",
    "        token_data.append(word_tokenize(english_data[i]))\n",
    "\n",
    "POS_data = []\n",
    "for i in range(len(token_data)):\n",
    "    temp = nltk.pos_tag(token_data[i])\n",
    "    POS_data.extend(temp)\n",
    "\n",
    "        \n",
    "#(b)\n",
    "from nltk.collocations import *\n",
    "\n",
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(POS_data)\n",
    "\n",
    "bigram_freq = finder.ngram_fd.items()\n",
    "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def rightTypes(ngram):\n",
    "    if '-pron-' in ngram or '' in ngram or ' 'in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords:\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    if ngram[0][1] in acceptable_types and ngram[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#Freuency filter\n",
    "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n",
    "freq_bi = filtered_bi[:20].bigram.values\n",
    "\n",
    "#PMI\n",
    "finder.apply_freq_filter(20)\n",
    "bigramPMITable = pd.DataFrame(list(finder.score_ngrams(bigrams.pmi)), columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)\n",
    "filteredPMI_bi = bigramPMITable[bigramPMITable.bigram.map(lambda x: rightTypes(x))]\n",
    "pmi_bi = filteredPMI_bi[:20].bigram.values\n",
    "\n",
    "#T-test\n",
    "bigramTtable = pd.DataFrame(list(finder.score_ngrams(bigrams.student_t)), columns=['bigram','t']).sort_values(by='t', ascending=False)\n",
    "filteredT_bi = bigramTtable[bigramTtable.bigram.map(lambda x: rightTypes(x))]\n",
    "t_bi = filteredT_bi[:20].bigram.values\n",
    "\n",
    "#Chi-squared\n",
    "bigramChiTable = pd.DataFrame(list(finder.score_ngrams(bigrams.chi_sq)), columns=['bigram','chi-sq']).sort_values(by='chi-sq', ascending=False)\n",
    "filteredChi_bi = bigramChiTable[bigramChiTable.bigram.map(lambda x: rightTypes(x))]\n",
    "chi_bi = filteredChi_bi[:20].bigram.values\n",
    "\n",
    "#DataFrame showing top20 results for each method\n",
    "df = pd.DataFrame({'Freq': freq_bi, 'PMI': pmi_bi, 't-test': t_bi, 'Chi-squared': chi_bi})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) There is quite a bit of overlap between the Frequency Filter method and the t-test method, so much so that both methods result in the same top 20 with 4 of them arranged differently. Meanwhile for chi-squared test and PMI methods there is a bit of overlap between but not nearly as much as between the other two.  \n",
    "\n",
    "I think that the union of the results would only make sense if there was a lot more filtering for actual words and if the duplicates were dropped.  \n",
    "    \n",
    "It is also apparent that the PMI and chi-squared methods are more likely to return collocations that aren't true English words, likely because those pairs of words only appear a few times (or even only once) together and never apart as is common with names of people, businesses, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10406)\t0.027656000883602202\n",
      "  (0, 12057)\t0.23896460811799206\n",
      "  (0, 28092)\t0.3149306440665282\n",
      "  (0, 23075)\t0.10944076525935781\n",
      "  (0, 12056)\t0.1574653220332641\n",
      "  (0, 27500)\t0.027656000883602202\n",
      "  (0, 23375)\t0.0364886860137175\n",
      "  (0, 14617)\t0.12798893497608843\n",
      "  (0, 3258)\t0.29310153070097117\n",
      "  (0, 15987)\t0.027680507753512076\n",
      "  (0, 20098)\t0.014322578193076913\n",
      "  (0, 31073)\t0.1757569590426551\n",
      "  (0, 5268)\t0.05603833183812126\n",
      "  (0, 13276)\t0.043153156492438526\n",
      "  (0, 1854)\t0.047129459740871044\n",
      "  (0, 1631)\t0.08877091822013107\n",
      "  (0, 26384)\t0.1557362589853452\n",
      "  (0, 26364)\t0.1557362589853452\n",
      "  (0, 18261)\t0.21893398457983754\n",
      "  (0, 31850)\t0.041074020985487694\n",
      "  (0, 6748)\t0.05561282132848242\n",
      "  (0, 31247)\t0.07555810553614567\n",
      "  (0, 1454)\t0.05138559360146931\n",
      "  (0, 11141)\t0.05528159590104083\n",
      "  (0, 12930)\t0.1038925794123324\n",
      "  :\t:\n",
      "  (3385, 27648)\t0.07849951643602643\n",
      "  (3385, 14430)\t0.07939764707109795\n",
      "  (3385, 13744)\t0.07288536604497745\n",
      "  (3385, 9515)\t0.07643032638053783\n",
      "  (3385, 4189)\t0.20454195870947303\n",
      "  (3385, 4578)\t0.10517523156074654\n",
      "  (3385, 12378)\t0.09756345603406828\n",
      "  (3385, 15543)\t0.0641701338660472\n",
      "  (3385, 18096)\t0.07778341257047734\n",
      "  (3385, 11847)\t0.1119593534612991\n",
      "  (3385, 25627)\t0.14001913177502018\n",
      "  (3385, 22563)\t0.08100028122025671\n",
      "  (3385, 25996)\t0.12868927197393545\n",
      "  (3385, 9280)\t0.09756345603406828\n",
      "  (3385, 27172)\t0.1265387530172809\n",
      "  (3385, 4057)\t0.10788512974578549\n",
      "  (3385, 29294)\t0.11603357717681272\n",
      "  (3385, 4153)\t0.11721194138153321\n",
      "  (3385, 3789)\t0.2239187069225982\n",
      "  (3385, 13370)\t0.13727074305554454\n",
      "  (3385, 15431)\t0.14134496677105815\n",
      "  (3385, 15582)\t0.14134496677105815\n",
      "  (3385, 3492)\t0.14659755469129226\n",
      "  (3385, 3491)\t0.29319510938258453\n",
      "  (3385, 30500)\t0.14659755469129226\n",
      "3386\n",
      "SVM confusion matrix:\n",
      " [[  0 239   0   0]\n",
      " [  0 286   0   0]\n",
      " [  0 301   0   0]\n",
      " [  0 190   0   0]]\n",
      "NB confusion matrix:\n",
      " [[204  13  15   7]\n",
      " [ 16 249  14   7]\n",
      " [ 15  20 258   8]\n",
      " [ 83  11  26  70]]\n",
      "\n",
      "SVM score: 0.281496062992126   NB score: 0.7687007874015748\n",
      "\n",
      "kernel scores:\n",
      "rbf: 0.281496062992126 linear: 0.8198818897637795\n",
      "Yes the different kernels do affect the accuracy, especially between the linear (highest score) and rbf (default)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ord() expected a character, but string of length 4 found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-aa7391519d87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[0mPOS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPOS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[0mPOS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPOS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_removeNonAscii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[0mPOS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPOS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mPOS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcorrectTypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[0mPOS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPOS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3380\u001b[0m         \"\"\"\n\u001b[0;32m   3381\u001b[0m         new_values = super(Series, self)._map_values(\n\u001b[1;32m-> 3382\u001b[1;33m             arg, na_action=na_action)\n\u001b[0m\u001b[0;32m   3383\u001b[0m         return self._constructor(new_values,\n\u001b[0;32m   3384\u001b[0m                                  index=self.index).__finalize__(self)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1218\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-aa7391519d87>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[0mPOS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPOS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[0mPOS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPOS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_removeNonAscii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[0mPOS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPOS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mPOS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcorrectTypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[0mPOS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPOS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-31ecceb958f5>\u001b[0m in \u001b[0;36m_removeNonAscii\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_removeNonAscii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m123\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m96\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m91\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mclean_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_removeNonAscii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-31ecceb958f5>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_removeNonAscii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m123\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m96\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m91\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mclean_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_removeNonAscii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ord() expected a character, but string of length 4 found"
     ]
    }
   ],
   "source": [
    "#2\n",
    "#(a)\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Because I have already removed numbers non-letter characters I will just remove stopwords and stem the words now\n",
    "ps = PorterStemmer()\n",
    "stemmed = []\n",
    "for i in range(len(token_data)):\n",
    "    temp = []\n",
    "    for w in token_data[i]:\n",
    "        if w not in en_stopwords:\n",
    "            temp.append(ps.stem(w).lower())\n",
    "    stemmed.append(temp)\n",
    "\n",
    "#(b)\n",
    "#making a callable tokenizer function to bypass the Tfidfvectorizer's tokenization because my data is already tokenized\n",
    "def tokenize(text):\n",
    "    return text\n",
    "vect = TfidfVectorizer(tokenizer=tokenize, lowercase=False)\n",
    "X = vect.fit_transform(stemmed)\n",
    "\n",
    "#(c)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "#I have found that the document at index 87 was deleted because it wasn't in English\n",
    "target = twenty_data.target[np.arange(len(twenty_data.target))!=87]\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(X, target, test_size=0.3)\n",
    "\n",
    "svm = SVC(gamma='scale', kernel='rbf').fit(X_train, y_train)\n",
    "pred_svm = svm.predict(X_test)\n",
    "matrix_svm = confusion_matrix(y_test, pred_svm)\n",
    "print(\"SVM confusion matrix:\\n\", matrix_svm)\n",
    "\n",
    "nb = MultinomialNB().fit(X_train, y_train)\n",
    "pred_nb = nb.predict(X_test)\n",
    "matrix_nb = confusion_matrix(y_test, pred_nb)\n",
    "print(\"NB confusion matrix:\\n\", matrix_nb)\n",
    "\n",
    "svm_score = accuracy_score(y_test, pred_svm)\n",
    "nb_score = accuracy_score(y_test, pred_nb)\n",
    "print(\"\\nSVM score:\",svm_score,\"  NB score:\",nb_score)\n",
    "\n",
    "svm2 = SVC(gamma='scale', kernel='linear').fit(X_train, y_train)\n",
    "pred_svm2 = svm2.predict(X_test)\n",
    "svm_score2 = accuracy_score(y_test, pred_svm2)\n",
    "print(\"\\nkernel scores:\\nrbf:\",svm_score, \"linear:\",svm_score2)\n",
    "print(\"Yes the different kernels do affect the accuracy, especially between the linear (highest score) and rbf (default)\")\n",
    "\n",
    "\n",
    "#(d)\n",
    "tokens = []\n",
    "for i in range(len(data)):\n",
    "    tokens.append(word_tokenize(data[i]))\n",
    "\n",
    "POS = []\n",
    "for i in range(len(tokens)):\n",
    "    temp = nltk.pos_tag(tokens[i])\n",
    "    POS.extend(temp)\n",
    "    \n",
    "def correctTypes(text):\n",
    "    if '-pron-' in text or '' in text or ' 'in text or 't' in text:\n",
    "        return False\n",
    "    for word in text:\n",
    "        if word in en_stopwords:\n",
    "            return False\n",
    "    acceptable_types = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    if text[1] in acceptable_types:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "POS = pd.Series(POS)\n",
    "POS = POS.map(lambda x: _removeNonAscii(x))\n",
    "POS = POS[POS.map(lambda x: correctTypes(x))]\n",
    "POS = POS.tolist()\n",
    "    \n",
    "stems = []\n",
    "for i in range(len(POS)):\n",
    "    temp = []\n",
    "    for w in POS[i]:\n",
    "        if w not in en_stopwords:\n",
    "            temp.append(ps.stem(w).lower())\n",
    "    stems.append(temp)\n",
    "\n",
    "X2 = vect.fit_transform(stems)\n",
    "\n",
    "#Repeat of question 'c'\n",
    "X_train2, X_test2, y_train2, y_test2 = tts(X2, target, test_size=0.3)\n",
    "\n",
    "svm_2 = SVC(gamma='scale', kernel='rbf').fit(X_train2, y_train2)\n",
    "pred_svm_2 = svm_2.predict(X_test2)\n",
    "matrix_svm_2 = confusion_matrix(y_test2, pred_svm_2)\n",
    "print(\"SVM confusion matrix:\\n\", matrix_svm_2)\n",
    "\n",
    "nb_2 = MultinomialNB().fit(X_train2, y_train2)\n",
    "pred_nb_2 = nb_2.predict(X_test2)\n",
    "matrix_nb_2 = confusion_matrix(y_test2, pred_nb_2)\n",
    "print(\"NB confusion matrix:\\n\", matrix_nb)\n",
    "\n",
    "svm_score_2 = accuracy_score(y_test2, pred_svm_2)\n",
    "nb_score_2 = accuracy_score(y_test2, pred_nb_2)\n",
    "print(\"\\nSVM score:\",svm_score_2,\"  NB score:\",nb_score_2)\n",
    "\n",
    "#The accuracy for the text classification is much higher when only the nouns are being used\n",
    "print(\"\\n# of words with nouns:\", len(POS_data), \"   # of words without nouns:\", len(POS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know what I messed up in the last part resulting in that error but I had it working before and could see that the accuracy when just nouns are being classified was much higher while the vocabulary was actually larger than before, but this could have been caused by the post-cleaning versus pre-cleaning but more likely was the error I was trying to fix and screwed up my whole code the day before it was due."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
